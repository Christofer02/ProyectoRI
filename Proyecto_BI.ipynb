{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "__LXn3NY8Tec",
        "outputId": "25a2367f-6186-47ef-f6b6-8325325c3b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.10.22)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V4O883je9JO7",
        "outputId": "3625f75d-bc1f-445e-c6ea-157b367dd1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Division"
      ],
      "metadata": {
        "id": "KrS2jANb_puk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "w7Kdc-v7hO1r",
        "outputId": "b22c8a71-2efe-41bb-ca56-c8c4f418576e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0e8b6979-0038-4115-a282-e8865852dba7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0e8b6979-0038-4115-a282-e8865852dba7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1429_1.csv to 1429_1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-f67aeedb7004>:10: DtypeWarning: Columns (1,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('1429_1.csv')\n"
          ]
        }
      ],
      "source": [
        "# Paso 1: Subir el archivo\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Paso 2: Leer el archivo CSV\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reemplaza con el nombre de tu archivo subido si es distinto\n",
        "df = pd.read_csv('1429_1.csv')\n",
        "\n",
        "# Paso 3: Filtrar columnas necesarias\n",
        "columns_to_keep = [\n",
        "    'name',\n",
        "    'categories',\n",
        "    'reviews.date',\n",
        "    'reviews.dateAdded',\n",
        "    'reviews.dateSeen',\n",
        "    'reviews.doRecommend',\n",
        "    'reviews.rating',\n",
        "    'reviews.text',\n",
        "    'reviews.title',\n",
        "    'reviews.username'\n",
        "]\n",
        "df_cleaned = df[columns_to_keep].copy()\n",
        "\n",
        "# Paso 4: Asignar precios estimados basados en el nombre o categoría\n",
        "def assign_price(row):\n",
        "    name = str(row['name']).lower()\n",
        "    categories = str(row['categories']).lower()\n",
        "\n",
        "    if 'paperwhite' in name or 'paperwhite' in categories:\n",
        "        return 159.99\n",
        "    elif 'signature' in name:\n",
        "        return 199.99\n",
        "    elif 'scribe' in name:\n",
        "        return 399.99\n",
        "    elif 'fire max' in name:\n",
        "        return 174.99\n",
        "    elif 'fire hd 8' in name or 'hd 8' in name:\n",
        "        return 99.99\n",
        "    elif 'fire 7' in name or 'tablet' in categories:\n",
        "        return 49.99\n",
        "    elif 'echo dot' in name:\n",
        "        return 49.99\n",
        "    elif 'echo' in name:\n",
        "        return 99.99\n",
        "    elif 'kindle' in name or 'kindle' in categories:\n",
        "        return 109.99\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "df_cleaned['Price'] = df_cleaned.apply(assign_price, axis=1)\n",
        "\n",
        "# Paso 5: Guardar el archivo limpio\n",
        "#output_filename = '1429_1_cleaned_with_prices.csv'\n",
        "#df_cleaned.to_csv(output_filename, index=False)\n",
        "\n",
        "# Paso 6: Descargar el nuevo archivo\n",
        "#files.download(output_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_cleaned.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n6EYYoo_br8",
        "outputId": "ac4e0af4-ca09-47dd-eb8e-740fb06aef06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 34660 entries, 0 to 34659\n",
            "Data columns (total 11 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   name                 27900 non-null  object \n",
            " 1   categories           34660 non-null  object \n",
            " 2   reviews.date         34621 non-null  object \n",
            " 3   reviews.dateAdded    24039 non-null  object \n",
            " 4   reviews.dateSeen     34660 non-null  object \n",
            " 5   reviews.doRecommend  34066 non-null  object \n",
            " 6   reviews.rating       34627 non-null  float64\n",
            " 7   reviews.text         34659 non-null  object \n",
            " 8   reviews.title        34654 non-null  object \n",
            " 9   reviews.username     34653 non-null  object \n",
            " 10  Price                34625 non-null  float64\n",
            "dtypes: float64(2), object(9)\n",
            "memory usage: 2.9+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 2.1 (Parte de la Etapa 2): Convertir 'reviews.date' a datetime\n",
        "# Asegúrate de que 'reviews.date' está en df_cleaned\n",
        "if 'reviews.date' in df_cleaned.columns:\n",
        "    df_cleaned['reviews.date'] = pd.to_datetime(df_cleaned['reviews.date'], errors='coerce')\n",
        "    print(\"\\nColumna 'reviews.date' convertida a datetime.\")\n",
        "    # Imprimir las primeras fechas y su tipo de dato para verificación\n",
        "    print(df_cleaned['reviews.date'].head())\n",
        "    print(df_cleaned['reviews.date'].dtype)\n",
        "else:\n",
        "    print(\"\\nLa columna 'reviews.date' no está presente en el DataFrame limpiado. No se realizó la conversión a datetime.\")\n",
        "\n",
        "# Paso 2.2 (Parte de la Etapa 2): Estandarizar el texto (minúsculas)\n",
        "text_cols_to_lower = ['reviews.text', 'reviews.title']\n",
        "for col in text_cols_to_lower:\n",
        "    if col in df_cleaned.columns:\n",
        "        # Usar fillna('') antes de astype(str) para evitar el string 'nan'\n",
        "        df_cleaned[f'{col}_lower'] = df_cleaned[col].fillna(\"\").astype(str).str.lower()\n",
        "        print(f\"\\nColumna '{col}' estandarizada (minúsculas).\")\n",
        "        print(df_cleaned[f'{col}_lower'].head())\n",
        "    else:\n",
        "        print(f\"\\nLa columna '{col}' no está presente en el DataFrame limpiado. No se realizó la estandarización.\")\n",
        "\n",
        "# print(df_cleaned.info())\n",
        "# print(df_cleaned[['reviews.date', 'reviews.text_lower', 'reviews.title_lower']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaINX1qgu9do",
        "outputId": "8d0f1717-2762-4e11-ddc0-2ad65480b162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Columna 'reviews.date' convertida a datetime.\n",
            "0   2017-01-13 00:00:00+00:00\n",
            "1   2017-01-13 00:00:00+00:00\n",
            "2   2017-01-13 00:00:00+00:00\n",
            "3   2017-01-13 00:00:00+00:00\n",
            "4   2017-01-12 00:00:00+00:00\n",
            "Name: reviews.date, dtype: datetime64[ns, UTC]\n",
            "datetime64[ns, UTC]\n",
            "\n",
            "Columna 'reviews.text' estandarizada (minúsculas).\n",
            "0    this product so far has not disappointed. my c...\n",
            "1    great for beginner or experienced person. boug...\n",
            "2    inexpensive tablet for him to use and learn on...\n",
            "3    i've had my fire hd 8 two weeks now and i love...\n",
            "4    i bought this for my grand daughter when she c...\n",
            "Name: reviews.text_lower, dtype: object\n",
            "\n",
            "Columna 'reviews.title' estandarizada (minúsculas).\n",
            "0                                     kindle\n",
            "1                                  very fast\n",
            "2    beginner tablet for our 9 year old son.\n",
            "3                                    good!!!\n",
            "4                  fantastic tablet for kids\n",
            "Name: reviews.title_lower, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_cleaned[['name', 'base_product_name']].sample(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn4jwMkt2N8S",
        "outputId": "d810f1bf-bcba-4b0d-c9c8-8a77ead5e462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    name base_product_name\n",
            "6617   Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...       Fire Tablet\n",
            "20809  Kindle Voyage E-reader, 6 High-Resolution Disp...   Kindle Voyage E\n",
            "9068   Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes...       Fire Tablet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'base_product_name' in df_cleaned.columns:\n",
        "    df_cleaned = df_cleaned.drop(columns=['base_product_name'])\n",
        "    print(\"Columna 'base_product_name' eliminada exitosamente.\")\n",
        "else:\n",
        "    print(\"La columna 'base_product_name' no se encuentra en el DataFrame.\")\n",
        "\n",
        "# Para verificar, puedes imprimir la lista de columnas\n",
        "# print(df_cleaned.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLpnJ_LZ1fMY",
        "outputId": "9558820d-1dde-468c-ffa0-e3f147c40cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columna 'base_product_name' eliminada exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "# Cargar el modelo de spaCy para inglés\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_lg\")\n",
        "    print(\"\\nModelo spaCy 'en_core_web_sm' cargado exitosamente para NER.\")\n",
        "except OSError:\n",
        "    print(\"\\nError: El modelo spaCy 'en_core_web_sm' no está instalado.\")\n",
        "    print(\"Por favor, ejecuta: !python -m spacy download en_core_web_sm\")\n",
        "    nlp = None # Asegurarse de que nlp es None si la carga falla\n",
        "\n",
        "\n",
        "# --- Paso 1 (Parte de la Etapa 2): Extraer base_product_name solo de la columna 'name' ---\n",
        "# Esta parte crea la columna inicialmente, resultando en nulos si 'name' estaba vacío/nulo\n",
        "def extract_base_from_name_only(name):\n",
        "    # Verificar si name es nulo o vacío después de limpiar espacios\n",
        "    if pd.isna(name) or not str(name).strip():\n",
        "        return None # Devuelve None si name está vacío/nulo\n",
        "\n",
        "    name_str = str(name)\n",
        "    prefixes_to_remove = [\n",
        "        r'^all-new\\s+',\n",
        "        r'^the\\s+'\n",
        "        # Añade otros patrones de prefijos aquí si es necesario\n",
        "    ]\n",
        "    cleaned_name_str = name_str.lower()\n",
        "    for prefix_pattern in prefixes_to_remove:\n",
        "        cleaned_name_str = re.sub(prefix_pattern, '', cleaned_name_str)\n",
        "\n",
        "    delimiter_pattern = r'([^,(-]+)'\n",
        "    match = re.match(delimiter_pattern, cleaned_name_str)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1).strip().title() # Opcional: Capitalizar\n",
        "    elif cleaned_name_str.strip(): # Si no hay delimitadores pero la cadena no está vacía\n",
        "         return cleaned_name_str.strip().title() # Opcional: Capitalizar\n",
        "    else:\n",
        "        return None # Devuelve None si la limpieza de name resulta en una cadena vacía\n",
        "\n",
        "if 'name' in df_cleaned.columns:\n",
        "    df_cleaned['base_product_name'] = df_cleaned['name'].apply(extract_base_from_name_only)\n",
        "    print(\"\\nNombre base del producto extraído inicialmente de la columna 'name'.\")\n",
        "    print(df_cleaned[['name', 'base_product_name']].head())\n",
        "else:\n",
        "    print(\"\\nLa columna 'name' no está presente. No se pudo extraer el nombre base de 'name'.\")\n",
        "\n",
        "\n",
        "# --- Paso 2 (Parte de la Etapa 2): Rellenar los valores nulos en 'base_product_name' ---\n",
        "\n",
        "# Define patrones de nombres de productos conocidos para buscar en el texto (para Regex)\n",
        "# Deben estar en minúsculas ya que reviews.text_lower está en minúsculas\n",
        "known_product_patterns_regex = {\n",
        "    'kindle paperwhite': r'kindle paperwhite',\n",
        "    'kindle signature': r'kindle signature',\n",
        "    'kindle scribe': r'kindle scribe',\n",
        "    'fire max': r'fire max',\n",
        "    'fire hd 8': r'fire hd 8|hd 8',\n",
        "    'fire 7': r'fire 7',\n",
        "    'echo dot': r'echo dot',\n",
        "    'echo': r'echo(?! dot)', # Echo que no sea Echo Dot\n",
        "    'kindle': r'kindle(?! paperwhite| signature| scribe)', # Kindle que no sea los otros\n",
        "    'tablet': r'tablet(?! fire)' # Ejemplo: buscar 'tablet' si no es parte de 'fire tablet'\n",
        "    # Añade aquí más nombres de productos o patrones que esperas encontrar con Regex\n",
        "}\n",
        "\n",
        "def fill_base_product_from_text(row):\n",
        "    current_base_name = row['base_product_name']\n",
        "\n",
        "    # 1. Intentar rellenar con Regex si base_product_name es nulo\n",
        "    if pd.isna(current_base_name):\n",
        "        review_text_lower = row.get('reviews.text_lower', \"\")\n",
        "        if review_text_lower and not pd.isna(review_text_lower):\n",
        "            found_in_text_regex = []\n",
        "            for product_name, pattern in known_product_patterns_regex.items():\n",
        "                if re.search(pattern, review_text_lower):\n",
        "                    found_in_text_regex.append(product_name.title())\n",
        "\n",
        "            if found_in_text_regex:\n",
        "                # Si Regex encontró algo, usarlo y detenerse\n",
        "                return found_in_text_regex[0]\n",
        "\n",
        "    # 2. Si base_product_name sigue siendo nulo (no encontrado por Name o Regex), intentar con spaCy\n",
        "    if pd.isna(current_base_name) and nlp and 'reviews.text_lower' in row: # Asegurarse de que nlp se cargó y la columna existe en la fila\n",
        "        review_text_lower = row.get('reviews.text_lower', \"\")\n",
        "        if review_text_lower and not pd.isna(review_text_lower):\n",
        "            doc = nlp(review_text_lower)\n",
        "            # Buscar entidades que probablemente sean nombres de productos con spaCy\n",
        "            # Ajusta los tipos de entidad según lo que sea relevante en tu dataset\n",
        "            product_entities_spacy = [\n",
        "                ent.text for ent in doc.ents if ent.label_ in ['PRODUCT', 'ORG', 'GPE', 'PERSON'] # 'PERSON' a veces identifica nombres propios que podrían ser productos\n",
        "            ]\n",
        "\n",
        "            if product_entities_spacy:\n",
        "                 # Devolvemos la primera entidad de producto/similar encontrada por spaCy\n",
        "                 # Podrías añadir lógica para filtrar o priorizar si hay múltiples coincidencias\n",
        "                return product_entities_spacy[0].strip().title() # Limpiar y Capitalizar\n",
        "\n",
        "\n",
        "    # Si después de Name, Regex y spaCy, base_product_name sigue siendo nulo, o si ya tenía un valor,\n",
        "    # devolvemos el valor actual\n",
        "    return current_base_name\n",
        "\n",
        "\n",
        "# Aplicar la función de relleno a todo el DataFrame.\n",
        "# La función internamente decide qué método usar según si el valor es nulo.\n",
        "if 'reviews.text_lower' in df_cleaned.columns and 'base_product_name' in df_cleaned.columns and nlp:\n",
        "     df_cleaned['base_product_name'] = df_cleaned.apply(\n",
        "        fill_base_product_from_text,\n",
        "        axis=1\n",
        "    )\n",
        "     print(\"\\nIntentando rellenar 'base_product_name' usando Name, luego Regex, luego spaCy (cuando era nulo).\")\n",
        "     # Muestra las columnas relevantes para ver si se rellenaron algunos valores\n",
        "     print(df_cleaned[['name', 'reviews.text', 'base_product_name']].head(10))\n",
        "\n",
        "elif not nlp:\n",
        "     print(\"\\nEl modelo spaCy no se cargó. No se usó NER para rellenar 'base_product_name'.\")\n",
        "else:\n",
        "     print(\"\\nLas columnas 'reviews.text_lower' o 'base_product_name' no están presentes. No se pudo intentar rellenar 'base_product_name'.\")\n",
        "\n",
        "\n",
        "# Verificar cuántos valores nulos quedaron después de todos los pasos\n",
        "print(f\"\\nValores nulos restantes en 'base_product_name': {df_cleaned['base_product_name'].isnull().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4quNvph89BM",
        "outputId": "f119d891-2eb6-4ffd-995b-37ffe8868ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Modelo spaCy 'en_core_web_sm' cargado exitosamente para NER.\n",
            "\n",
            "Nombre base del producto extraído inicialmente de la columna 'name'.\n",
            "                                                name base_product_name\n",
            "0  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Fire Hd 8 Tablet\n",
            "1  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Fire Hd 8 Tablet\n",
            "2  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Fire Hd 8 Tablet\n",
            "3  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Fire Hd 8 Tablet\n",
            "4  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Fire Hd 8 Tablet\n",
            "\n",
            "Intentando rellenar 'base_product_name' usando Name, luego Regex, luego spaCy (cuando era nulo).\n",
            "                                                name  \\\n",
            "0  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "1  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "2  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "3  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "4  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "5  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "6  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "7  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "8  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "9  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
            "\n",
            "                                        reviews.text base_product_name  \n",
            "0  This product so far has not disappointed. My c...  Fire Hd 8 Tablet  \n",
            "1  great for beginner or experienced person. Boug...  Fire Hd 8 Tablet  \n",
            "2  Inexpensive tablet for him to use and learn on...  Fire Hd 8 Tablet  \n",
            "3  I've had my Fire HD 8 two weeks now and I love...  Fire Hd 8 Tablet  \n",
            "4  I bought this for my grand daughter when she c...  Fire Hd 8 Tablet  \n",
            "5  This amazon fire 8 inch tablet is the perfect ...  Fire Hd 8 Tablet  \n",
            "6  Great for e-reading on the go, nice and light ...  Fire Hd 8 Tablet  \n",
            "7  I gave this as a Christmas gift to my inlaws, ...  Fire Hd 8 Tablet  \n",
            "8  Great as a device to read books. I like that i...  Fire Hd 8 Tablet  \n",
            "9  I love ordering books and reading them with th...  Fire Hd 8 Tablet  \n",
            "\n",
            "Valores nulos restantes en 'base_product_name': 4129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etapa 4"
      ],
      "metadata": {
        "id": "odsf3prYK-Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El modelo spaCy 'nlp' debería estar cargado desde la celda anterior (donde se usó para rellenar base_product_name)\n",
        "# Verificamos si nlp no es None\n",
        "if nlp and 'reviews.text_lower' in df_cleaned.columns:\n",
        "\n",
        "    # Define verbos clave para diferentes tipos de eventos (usa lemas si es posible para mejor coincidencia)\n",
        "    # Puedes ampliar estas listas\n",
        "    function_verbs = ['work', 'function', 'perform'] # Verbos relacionados con el buen funcionamiento\n",
        "    problem_verbs = ['fail', 'break', 'stop', 'issue', 'problem'] # Verbos relacionados con problemas/fallos\n",
        "    recommend_verbs = ['recommend'] # Verbos relacionados con recomendación\n",
        "    buy_verbs = ['buy', 'purchase', 'get'] # Verbos relacionados con la compra\n",
        "\n",
        "    def extract_events(text):\n",
        "        events = []\n",
        "        if pd.isna(text) or not text.strip():\n",
        "            return events # Devuelve lista vacía si el texto es nulo/vacío\n",
        "\n",
        "        # Procesar el texto con spaCy para obtener el objeto Doc\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Itera sobre los tokens en el documento procesado por spaCy\n",
        "        for token in doc:\n",
        "            # Busca verbos clave\n",
        "            if token.pos_ == 'VERB': # Si el token es un verbo\n",
        "                lemma = token.lemma_.lower() # Obtiene el lema en minúsculas\n",
        "\n",
        "                event_type = None\n",
        "                # Verifica a qué tipo de evento pertenece el verbo\n",
        "                if lemma in function_verbs:\n",
        "                    event_type = 'funcionamiento'\n",
        "                elif lemma in problem_verbs:\n",
        "                    event_type = 'problema/fallo'\n",
        "                elif lemma in recommend_verbs:\n",
        "                    event_type = 'recomendacion'\n",
        "                elif lemma in buy_verbs:\n",
        "                     # También considera formas pasadas irregulares o frases\n",
        "                     # Usamos token.text.lower() para verificar la forma original del verbo\n",
        "                     if lemma == 'buy' and token.text.lower() in ['bought', 'buy', 'buys', 'buying']:\n",
        "                         event_type = 'compra'\n",
        "                     elif lemma == 'get' and token.text.lower() in ['got', 'get', 'gets', 'getting']:\n",
        "                         event_type = 'compra'\n",
        "                     elif lemma == 'purchase' and token.text.lower() in ['purchased', 'purchase', 'purchases', 'purchasing']:\n",
        "                         event_type = 'compra'\n",
        "\n",
        "\n",
        "                if event_type:\n",
        "                    # Aquí podrías añadir lógica más compleja para identificar\n",
        "                    # sujeto, objeto, etc., usando dependencias sintácticas (token.dep_, token.head, etc.)\n",
        "                    # Por ahora, simplemente registraremos el verbo y su tipo de evento\n",
        "                    event_details = {\n",
        "                        'type': event_type,\n",
        "                        'verb': token.text, # La forma original del verbo en el texto\n",
        "                        'lemma': lemma,      # El lema del verbo\n",
        "                        'start': token.idx,  # Posición inicial del verbo en el texto\n",
        "                        'end': token.idx + len(token.text) # Posición final\n",
        "                        # Puedes añadir aquí otras informaciones relevantes\n",
        "                    }\n",
        "                    events.append(event_details)\n",
        "\n",
        "        return events\n",
        "\n",
        "    # Aplica la función a la columna de texto estandarizada\n",
        "    # Procesar eventos puede ser intensivo. Considera procesar una muestra si tu dataset es muy grande.\n",
        "    # df_cleaned['extracted_events'] = df_cleaned['reviews.text_lower'].head(100).apply(extract_events)\n",
        "    df_cleaned['extracted_events'] = df_cleaned['reviews.text_lower'].apply(extract_events)\n",
        "\n",
        "    print(\"\\nExtracción de Eventos completada.\")\n",
        "    # Muestra las columnas relevantes para ver los eventos extraídos\n",
        "    print(df_cleaned[['reviews.text', 'extracted_events']].head())\n",
        "\n",
        "    # Opcional: Ver la distribución de tipos de eventos\n",
        "    # Aplanar la lista de listas de eventos\n",
        "    all_event_types = [event['type'] for events_list in df_cleaned['extracted_events'] for event in events_list]\n",
        "    if all_event_types:\n",
        "        event_type_counts = pd.Series(all_event_types).value_counts()\n",
        "        print(\"\\nDistribución de tipos de eventos extraídos:\")\n",
        "        print(event_type_counts)\n",
        "\n",
        "else:\n",
        "    if not nlp:\n",
        "        print(\"\\nNo se pudo cargar el modelo spaCy. La extracción de Eventos no se realizó.\")\n",
        "    elif 'reviews.text_lower' not in df_cleaned.columns:\n",
        "        print(\"\\nLa columna 'reviews.text_lower' no está presente.\")\n",
        "\n",
        "# Ahora df_cleaned tiene una nueva columna 'extracted_events' con una lista de eventos encontrados en cada reseña."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bNjYO2TW82S",
        "outputId": "a6bba9bd-a1b4-44e1-dc89-bc923149ffbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracción de Eventos completada.\n",
            "                                        reviews.text  \\\n",
            "0  This product so far has not disappointed. My c...   \n",
            "1  great for beginner or experienced person. Boug...   \n",
            "2  Inexpensive tablet for him to use and learn on...   \n",
            "3  I've had my Fire HD 8 two weeks now and I love...   \n",
            "4  I bought this for my grand daughter when she c...   \n",
            "\n",
            "                                    extracted_events  \n",
            "0                                                 []  \n",
            "1  [{'type': 'compra', 'verb': 'bought', 'lemma':...  \n",
            "2                                                 []  \n",
            "3                                                 []  \n",
            "4  [{'type': 'compra', 'verb': 'bought', 'lemma':...  \n",
            "\n",
            "Distribución de tipos de eventos extraídos:\n",
            "compra            15779\n",
            "funcionamiento     4624\n",
            "recomendacion      2203\n",
            "problema/fallo      593\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_cleaned[[ 'extracted_events']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ1jd9fRcREc",
        "outputId": "09c0cd91-7e4a-40ed-b07b-134dbb6b9cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    extracted_events\n",
            "0                                                 []\n",
            "1  [{'type': 'compra', 'verb': 'bought', 'lemma':...\n",
            "2                                                 []\n",
            "3                                                 []\n",
            "4  [{'type': 'compra', 'verb': 'bought', 'lemma':...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etapa 5"
      ],
      "metadata": {
        "id": "XYPc8MRIrx2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El modelo spaCy 'nlp' debería estar cargado desde una celda anterior.\n",
        "if nlp and 'reviews.text_lower' in df_cleaned.columns and 'base_product_name' in df_cleaned.columns and 'reviews.username' in df_cleaned.columns and 'reviews.doRecommend' in df_cleaned.columns and 'reviews.rating' in df_cleaned.columns:\n",
        "\n",
        "    # Define verbos clave que a menudo indican relaciones donde el sujeto es el usuario\n",
        "    user_action_verbs = ['buy', 'purchase', 'get', 'return', 'send back', 'recommend', 'love', 'like', 'dislike', 'hate', 'use', 'try', 'experience']\n",
        "\n",
        "    # Define verbos clave que a menudo indican relaciones donde el sujeto es el producto\n",
        "    product_action_verbs = ['work', 'function', 'perform', 'fail', 'break', 'stop', 'charge', 'update']\n",
        "\n",
        "\n",
        "    def extract_relations_with_recommendation_and_sentiment(row):\n",
        "        relations = []\n",
        "        text = row.get('reviews.text_lower', \"\")\n",
        "        base_product = row.get('base_product_name')\n",
        "        username = row.get('reviews.username')\n",
        "        do_recommend = row.get('reviews.doRecommend')\n",
        "        rating = row.get('reviews.rating')\n",
        "\n",
        "        # --- 1. Extraer relaciones del texto (usando dependencias, como antes) ---\n",
        "        # Re-procesamos el texto si es necesario para obtener el objeto Doc\n",
        "        if pd.notna(text) and text.strip():\n",
        "            doc = nlp(text)\n",
        "\n",
        "            for token in doc:\n",
        "                if token.pos_ == 'VERB' and (token.lemma_.lower() in user_action_verbs or token.lemma_.lower() in product_action_verbs):\n",
        "                    verb_token = token\n",
        "                    verb_lemma = verb_token.lemma_.lower()\n",
        "\n",
        "                    subject = None\n",
        "                    obj = None\n",
        "\n",
        "                    for child in verb_token.children:\n",
        "                        if child.dep_ == 'nsubj' or child.dep_ == 'nsubjpass':\n",
        "                            subject = child.text.strip()\n",
        "                            break\n",
        "\n",
        "                    for child in verb_token.children:\n",
        "                        if child.dep_ == 'obj' or child.dep_ == 'dobj':\n",
        "                            obj = child.text.strip()\n",
        "                            break\n",
        "\n",
        "                    final_subject = None\n",
        "                    if verb_lemma in user_action_verbs and pd.notna(username) and str(username).strip():\n",
        "                         if subject is None or subject.lower() in ['i', 'we']:\n",
        "                              final_subject = username\n",
        "\n",
        "                    if final_subject is None:\n",
        "                         final_subject = subject\n",
        "\n",
        "                    final_obj = obj\n",
        "                    if verb_lemma in user_action_verbs and final_obj and final_obj.lower() in ['it', 'this', 'that'] and pd.notna(base_product):\n",
        "                         final_obj = base_product\n",
        "                    elif verb_lemma in product_action_verbs and final_subject and final_subject.lower() == 'it' and pd.notna(base_product):\n",
        "                         final_subject = base_product\n",
        "\n",
        "                    if final_subject or final_obj:\n",
        "                        relations.append({\n",
        "                            'subject': final_subject,\n",
        "                            'predicate': verb_lemma,\n",
        "                            'object': final_obj\n",
        "                        })\n",
        "\n",
        "        # --- 2. Añadir relación de Recomendación (basado en reviews.doRecommend) ---\n",
        "        # Solo añadimos la relación si hay un username válido y un base_product válido\n",
        "        if pd.notna(username) and str(username).strip() and pd.notna(base_product) and str(base_product).strip():\n",
        "            if pd.notna(do_recommend): # Verificar si el valor de doRecommend no es nulo\n",
        "                if do_recommend is True:\n",
        "                    relations.append({\n",
        "                        'subject': username,\n",
        "                        'predicate': 'recommends',\n",
        "                        'object': base_product\n",
        "                    })\n",
        "                elif do_recommend is False:\n",
        "                     relations.append({\n",
        "                        'subject': username,\n",
        "                        'predicate': 'does_not_recommend',\n",
        "                        'object': base_product\n",
        "                    })\n",
        "\n",
        "        # --- 3. Añadir relación de Sentimiento (basado en reviews.rating) ---\n",
        "        # Solo añadimos la relación si hay un username válido, un base_product válido y una calificación válida\n",
        "        if pd.notna(username) and str(username).strip() and pd.notna(base_product) and str(base_product).strip() and pd.notna(rating):\n",
        "            try:\n",
        "                rating_value = int(rating) # Intentar convertir la calificación a entero\n",
        "                sentiment = None\n",
        "                if rating_value >= 4 and rating_value <= 5:\n",
        "                    sentiment = 'has_positive_sentiment_towards'\n",
        "                elif rating_value == 3:\n",
        "                    sentiment = 'has_neutral_sentiment_towards'\n",
        "                elif rating_value >= 1 and rating_value <= 2:\n",
        "                    sentiment = 'has_negative_sentiment_towards'\n",
        "\n",
        "                if sentiment:\n",
        "                     relations.append({\n",
        "                        'subject': username,\n",
        "                        'predicate': sentiment,\n",
        "                        'object': base_product\n",
        "                    })\n",
        "            except ValueError:\n",
        "                # Manejar casos donde rating no es un número válido\n",
        "                print(f\"Advertencia: Calificación no válida encontrada: {rating}. Omitiendo relación de sentimiento.\")\n",
        "                pass # No hacer nada si la conversión falla\n",
        "\n",
        "\n",
        "        return relations\n",
        "\n",
        "    # Aplica la función a cada fila del DataFrame\n",
        "    # df_cleaned['extracted_relations_all'] = df_cleaned.head(100).apply(extract_relations_with_recommendation_and_sentiment, axis=1)\n",
        "    df_cleaned['extracted_relations_all'] = df_cleaned.apply(extract_relations_with_recommendation_and_sentiment, axis=1)\n",
        "\n",
        "\n",
        "    print(\"\\nExtracción de Relaciones completada (Incluyendo Recomendación y Sentimiento de Rating).\")\n",
        "    # Muestra las columnas relevantes\n",
        "    print(df_cleaned[['reviews.text', 'reviews.username', 'base_product_name', 'reviews.doRecommend', 'reviews.rating', 'extracted_relations_all']].head())\n",
        "\n",
        "    # Opcional: Aplanar y contar las relaciones\n",
        "    # all_relations_all = [rel for rels_list in df_cleaned['extracted_relations_all'] for rel in rels_list]\n",
        "    # if all_relations_all:\n",
        "    #     print(\"\\nPrimeras 10 relaciones extraídas (Todas las Fuentes):\")\n",
        "    #     for i, rel in enumerate(all_relations_all[:10]):\n",
        "    #         print(rel)\n",
        "\n",
        "else:\n",
        "     if not nlp:\n",
        "         print(\"\\nNo se pudo cargar el modelo spaCy. No se realizó la Extracción de Relaciones.\")\n",
        "     elif 'reviews.text_lower' not in df_cleaned.columns or 'base_product_name' not in df_cleaned.columns or 'reviews.username' not in df_cleaned.columns or 'reviews.doRecommend' not in df_cleaned.columns or 'reviews.rating' not in df_cleaned.columns:\n",
        "         print(\"\\nLas columnas requeridas no están presentes.\")\n",
        "\n",
        "# Ahora df_cleaned tiene una nueva columna 'extracted_relations_all' con todas las relaciones."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMkKXKDgvSv-",
        "outputId": "d53557c3-83dd-4e8d-c3d9-50dcda4f288f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracción de Relaciones completada (Incluyendo Recomendación y Sentimiento de Rating).\n",
            "                                        reviews.text reviews.username  \\\n",
            "0  This product so far has not disappointed. My c...          Adapter   \n",
            "1  great for beginner or experienced person. Boug...           truman   \n",
            "2  Inexpensive tablet for him to use and learn on...            DaveZ   \n",
            "3  I've had my Fire HD 8 two weeks now and I love...           Shacks   \n",
            "4  I bought this for my grand daughter when she c...        explore42   \n",
            "\n",
            "  base_product_name reviews.doRecommend  reviews.rating  \\\n",
            "0  Fire Hd 8 Tablet                True             5.0   \n",
            "1  Fire Hd 8 Tablet                True             5.0   \n",
            "2  Fire Hd 8 Tablet                True             5.0   \n",
            "3  Fire Hd 8 Tablet                True             4.0   \n",
            "4  Fire Hd 8 Tablet                True             5.0   \n",
            "\n",
            "                             extracted_relations_all  \n",
            "0  [{'subject': 'children', 'predicate': 'love', ...  \n",
            "1  [{'subject': 'truman', 'predicate': 'buy', 'ob...  \n",
            "2  [{'subject': 'him', 'predicate': 'use', 'objec...  \n",
            "3  [{'subject': 'Shacks', 'predicate': 'love', 'o...  \n",
            "4  [{'subject': 'explore42', 'predicate': 'buy', ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_cleaned[[ 'extracted_relations_all']].sample(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVYU8aNO1MhP",
        "outputId": "56fb4ff7-ed60-4476-dfa2-e4927d18e25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 extracted_relations_all\n",
            "4194   [{'subject': 'Bubbles8406', 'predicate': 'buy'...\n",
            "4137   [{'subject': 'CareBear', 'predicate': 'buy', '...\n",
            "30484  [{'subject': 'StevenA', 'predicate': 'purchase...\n",
            "9039   [{'subject': 'razorback', 'predicate': 'get', ...\n",
            "32893  [{'subject': 'Johny', 'predicate': 'recommend'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegúrate de que pandas y json están importados\n",
        "import pandas as pd\n",
        "import json # ¡Importar la librería json!\n",
        "# Importar la función files de google.colab\n",
        "from google.colab import files\n",
        "\n",
        "# Asumiendo que df_cleaned es el DataFrame que quieres descargar\n",
        "\n",
        "# --- MODIFICACIÓN CLAVE: SERIALIZAR COLUMNAS A JSON VÁLIDO ---\n",
        "# Aplica json.dumps() a las columnas que contienen listas de diccionarios\n",
        "# Asegúrate de que la columna existe y contiene listas antes de aplicar json.dumps\n",
        "if 'extracted_events' in df_cleaned.columns:\n",
        "    df_cleaned['extracted_events'] = df_cleaned['extracted_events'].apply(lambda x: json.dumps(x) if isinstance(x, list) else '[]')\n",
        "    print(\"Columna 'extracted_events' serializada a JSON válido.\")\n",
        "\n",
        "if 'extracted_relations_all' in df_cleaned.columns:\n",
        "    df_cleaned['extracted_relations_all'] = df_cleaned['extracted_relations_all'].apply(lambda x: json.dumps(x) if isinstance(x, list) else '[]')\n",
        "    print(\"Columna 'extracted_relations_all' serializada a JSON válido.\")\n",
        "\n",
        "# Define el nombre del archivo de salida\n",
        "output_filename = 'df_cleaned.csv' # Puedes cambiar este nombre\n",
        "\n",
        "# Guarda el DataFrame en un archivo CSV\n",
        "# index=False evita que pandas escriba el índice del DataFrame como una columna en el CSV\n",
        "# Ahora las columnas problemáticas ya son strings JSON válidos\n",
        "df_cleaned.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"DataFrame guardado localmente como '{output_filename}'.\")\n",
        "\n",
        "# Descarga el archivo a tu computadora local\n",
        "files.download(output_filename)\n",
        "\n",
        "print(f\"Descargando '{output_filename}'...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "2ItgICfL8DBr",
        "outputId": "a8e27dc6-b686-4540-f5ff-11e61bc5e114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columna 'extracted_events' serializada a JSON válido.\n",
            "Columna 'extracted_relations_all' serializada a JSON válido.\n",
            "DataFrame guardado localmente como 'df_cleaned.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ef412509-a40f-47da-8706-cf582cc4663b\", \"df_cleaned.csv\", 42974114)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando 'df_cleaned.csv'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegúrate de que pandas está importado\n",
        "import pandas as pd\n",
        "# Importar la función files de google.colab\n",
        "from google.colab import files\n",
        "\n",
        "# Asumiendo que df_cleaned es el DataFrame que quieres descargar\n",
        "\n",
        "# Define el nombre del archivo de salida\n",
        "output_filename = 'df_cleaned.csv' # Puedes cambiar este nombre\n",
        "\n",
        "# Guarda el DataFrame en un archivo CSV\n",
        "# index=False evita que pandas escriba el índice del DataFrame como una columna en el CSV\n",
        "df_cleaned.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"DataFrame guardado localmente como '{output_filename}'.\")\n",
        "\n",
        "# Descarga el archivo a tu computadora local\n",
        "files.download(output_filename)\n",
        "\n",
        "print(f\"Descargando '{output_filename}'...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "L5jPmITzL6VE",
        "outputId": "91f15f5d-1666-428b-9337-b1be91167c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame guardado localmente como 'df_cleaned.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_28c7a78f-3b41-44bd-8ce2-86e83e6f84c3\", \"df_cleaned.csv\", 41371401)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando 'df_cleaned.csv'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etapa 6"
      ],
      "metadata": {
        "id": "m4KdK1Gc1zej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "moO8_ufw12Dl",
        "outputId": "870a9c02-b4d8-4415-bb4a-9059c4f43f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib) (3.2.3)\n",
            "Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/565.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m563.2/565.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdflib\n",
            "Successfully installed rdflib-7.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph, Namespace, Literal, URIRef, BNode\n",
        "from rdflib.namespace import RDF, RDFS\n",
        "# Importar urlencode para codificación URL\n",
        "from urllib.parse import quote\n",
        "\n",
        "# 1. Crear un Grafo Vacío\n",
        "g = Graph()\n",
        "print(\"\\nGrafo RDF vacío creado.\")\n",
        "\n",
        "# 2. Definir un Vocabulario Básico (Namespaces y URIs)\n",
        "ex = Namespace(\"http://example.org/review-ontology#\")\n",
        "schema = Namespace(\"http://schema.org/\")\n",
        "g.bind(\"ex\", ex)\n",
        "g.bind(\"schema\", schema)\n",
        "g.bind(\"rdf\", RDF)\n",
        "g.bind(\"rdfs\", RDFS)\n",
        "print(\"Namespaces definidos y vinculados.\")\n",
        "\n",
        "# Función para crear una parte local de URI segura usando codificación URL\n",
        "def safe_uri_part(text):\n",
        "    if pd.isna(text) or not str(text).strip():\n",
        "        return None\n",
        "    # Usa quote para codificar caracteres especiales.\n",
        "    # El segundo argumento safe='' asegura que incluso los caracteres como '/' se codifiquen si aparecen.\n",
        "    # Podríamos mantener algunos caracteres \"seguros\" si sabemos que no causan problemas,\n",
        "    # pero codificar todo lo que no sea alfanumérico es más seguro.\n",
        "    # Reemplazar espacios con guiones bajos ANTES de codificar es opcional pero a veces preferido para legibilidad.\n",
        "    cleaned_text = str(text).strip().replace(\" \", \"_\")\n",
        "    return quote(cleaned_text, safe='') # Codifica todos los caracteres especiales\n",
        "\n",
        "\n",
        "# 3. Convertir Relaciones a Tripletas RDF y Añadir al Grafo\n",
        "# 4. Añadir la Relación Implícita de Compra\n",
        "\n",
        "if 'extracted_relations_all' in df_cleaned.columns and 'reviews.username' in df_cleaned.columns and 'base_product_name' in df_cleaned.columns:\n",
        "\n",
        "    triple_count = 0\n",
        "\n",
        "    for index, row in df_cleaned.iterrows():\n",
        "        username = row.get('reviews.username')\n",
        "        base_product = row.get('base_product_name')\n",
        "        relations = row.get('extracted_relations_all', [])\n",
        "\n",
        "        # Crear URI segura para el usuario\n",
        "        user_uri_part = safe_uri_part(username)\n",
        "        user_uri = ex[user_uri_part] if user_uri_part else None\n",
        "\n",
        "        # Crear URI segura para el producto\n",
        "        product_uri_part = safe_uri_part(base_product)\n",
        "        product_uri = ex[product_uri_part] if product_uri_part else None\n",
        "\n",
        "\n",
        "        # --- Añadir la Relación Implícita de Compra ---\n",
        "        if user_uri and product_uri:\n",
        "             g.add((user_uri, ex.bought, product_uri))\n",
        "             triple_count += 1\n",
        "             g.add((user_uri, RDF.type, schema.Person))\n",
        "             g.add((product_uri, RDF.type, schema.Product))\n",
        "             # La etiqueta RDFS.label puede ser un Literal, que no necesita codificación de URI\n",
        "             if pd.notna(base_product) and str(base_product).strip():\n",
        "                 g.add((product_uri, RDFS.label, Literal(base_product.strip())))\n",
        "\n",
        "\n",
        "        # --- Añadir Relaciones Extraídas de la Columna 'extracted_relations_all' ---\n",
        "        if isinstance(relations, list):\n",
        "            for relation in relations:\n",
        "                subject = relation.get('subject')\n",
        "                predicate_str = relation.get('predicate')\n",
        "                obj = relation.get('object')\n",
        "\n",
        "                s_uri = None\n",
        "                o_uri = None\n",
        "                p_uri = None\n",
        "\n",
        "                # Determinar URI del Sujeto\n",
        "                if pd.notna(subject) and str(subject).strip():\n",
        "                    if subject == username and user_uri:\n",
        "                        s_uri = user_uri\n",
        "                    elif subject == base_product and product_uri:\n",
        "                         s_uri = product_uri\n",
        "                    else:\n",
        "                         # Si es otro término, crear URI segura si es una cadena, o mantener como Literal\n",
        "                         if isinstance(subject, str):\n",
        "                             subject_uri_part = safe_uri_part(subject)\n",
        "                             # Intentar crear URI segura, si falla o no es una cadena, usar Literal\n",
        "                             s_uri = ex[subject_uri_part] if subject_uri_part else Literal(subject)\n",
        "                         else: # Si no es una cadena (ej: None, número), tratar como Literal\n",
        "                             s_uri = Literal(subject)\n",
        "\n",
        "\n",
        "                # Determinar URI/Literal del Objeto\n",
        "                if pd.notna(obj) and str(obj).strip():\n",
        "                    if obj == username and user_uri:\n",
        "                        o_uri = user_uri\n",
        "                    elif obj == base_product and product_uri:\n",
        "                         o_uri = product_uri\n",
        "                    else:\n",
        "                         # Similar al sujeto, tratar como Literal o intentar mapear a URI\n",
        "                         if isinstance(obj, str):\n",
        "                              obj_uri_part = safe_uri_part(obj)\n",
        "                              # Intentar crear URI segura, si falla o no es una cadena, usar Literal\n",
        "                              o_uri = ex[obj_uri_part] if obj_uri_part else Literal(obj)\n",
        "                         else: # Si no es una cadena, tratar como Literal\n",
        "                             o_uri = Literal(obj)\n",
        "\n",
        "                # Determinar URI del Predicado\n",
        "                if pd.notna(predicate_str) and str(predicate_str).strip():\n",
        "                    # Crear una URI segura para el predicado\n",
        "                    predicate_uri_part = safe_uri_part(predicate_str)\n",
        "                    # Intentar crear URI segura, si falla o la cadena está vacía/nula, el predicado es None\n",
        "                    p_uri = ex[predicate_uri_part] if predicate_uri_part else None\n",
        "\n",
        "\n",
        "                # Añadir la tripleta al grafo si Sujeto, Predicado y Objeto son válidos\n",
        "                # Aseguramos que Sujeto y Predicado sean tipos aceptables para rdflib.Graph.add\n",
        "                # Sujeto debe ser URIRef o BNode\n",
        "                # Predicado debe ser URIRef\n",
        "                # Objeto puede ser URIRef, Literal o BNode\n",
        "                if s_uri is not None and p_uri is not None and o_uri is not None: # Asegurarse de que los tres nodos no sean None de Python\n",
        "                    if isinstance(s_uri, (URIRef, BNode)) and isinstance(p_uri, URIRef): # Corregido: Usar BNode directamente\n",
        "                         g.add((s_uri, p_uri, o_uri))\n",
        "                         triple_count += 1\n",
        "                    # Si el Sujeto es Literal, a veces se puede modelar, pero lo estándar es URI/BNode como sujeto.\n",
        "                    # Por ahora, saltamos estas tripletas para adherirnos al modelado estándar.\n",
        "                    # print(f\"Advertencia: Saltando tripleta con Sujeto Literal: ({s_uri}, {p_uri}, {o_uri})\")\n",
        "\n",
        "\n",
        "    print(f\"\\nSe añadieron {triple_count} tripletas al grafo.\")\n",
        "    print(f\"Total de tripletas en el grafo: {len(g)}\")\n",
        "\n",
        "    # 5. Serializar el Grafo (Guardar)\n",
        "    output_graph_file = \"reviews_knowledge_graph.ttl\"\n",
        "    # Asegurarse de que todos los URIs son válidos antes de serializar\n",
        "    # La función safe_uri_part ayuda con esto, pero si aún hay problemas, podrías necesitar más limpieza.\n",
        "    g.serialize(destination=output_graph_file, format='turtle')\n",
        "\n",
        "    print(f\"\\nGrafo RDF guardado en '{output_graph_file}' (formato Turtle).\")\n",
        "    # Puedes descargar el archivo si estás en Colab\n",
        "    # from google.colab import files\n",
        "    # files.download(output_graph_file)\n",
        "\n",
        "else:\n",
        "     print(\"\\nLas columnas necesarias ('extracted_relations_all', 'reviews.username', 'base_product_name') no están presentes. No se pudo crear el grafo RDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gJKMNOn5qzn",
        "outputId": "0fe07364-356f-49a1-b6f6-19755df18b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Grafo RDF vacío creado.\n",
            "Namespaces definidos y vinculados.\n",
            "\n",
            "Se añadieron 120714 tripletas al grafo.\n",
            "Total de tripletas en el grafo: 131378\n",
            "\n",
            "Grafo RDF guardado en 'reviews_knowledge_graph.ttl' (formato Turtle).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etapa 7"
      ],
      "metadata": {
        "id": "m0F5Afas7aDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar el Grafo desde el archivo Turtle\n",
        "g = Graph()\n",
        "graph_file = \"reviews_knowledge_graph.ttl\"\n",
        "\n",
        "try:\n",
        "    g.parse(graph_file, format=\"turtle\")\n",
        "    print(f\"\\nGrafo RDF cargado exitosamente desde '{graph_file}'.\")\n",
        "    print(f\"Total de tripletas en el grafo cargado: {len(g)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: El archivo '{graph_file}' no fue encontrado.\")\n",
        "    print(\"Asegúrate de haber ejecutado la Etapa 6 primero para crear el archivo.\")\n",
        "    g = None # Asegurarse de que g es None si la carga falla\n",
        "\n",
        "\n",
        "if g:\n",
        "    # 2. Definir los Namespaces para usar en las consultas SPARQL\n",
        "    # Es importante que estos prefixes coincidan con los que usaste al crear el grafo\n",
        "    ex = Namespace(\"http://example.org/review-ontology#\")\n",
        "    schema = Namespace(\"http://schema.org/\")\n",
        "\n",
        "    # 3. Escribir y Ejecutar Consultas SPARQL\n",
        "\n",
        "    # --- Consulta 1: Obtener todos los productos con sentimiento positivo ---\n",
        "    print(\"\\n--- Consulta 1: Productos con Sentimiento Positivo ---\")\n",
        "    # SELECT ?product ?user WHERE { ... } - Selecciona las variables que quieres ver\n",
        "    # WHERE { ... } - Define los patrones de tripletas a buscar\n",
        "    # ?user ex:has_positive_sentiment_towards ?product . - Busca tripletas donde el predicado es ex:has_positive_sentiment_towards\n",
        "    #                                                      y vincula el sujeto a ?user y el objeto a ?product.\n",
        "    # OPTIONAL { ?product rdfs:label ?label } - Opcional: Si el producto tiene una etiqueta rdfs:label, vincularla a ?label\n",
        "    query1 = \"\"\"\n",
        "    SELECT ?user ?product ?label\n",
        "    WHERE {\n",
        "        ?user ex:has_positive_sentiment_towards ?product .\n",
        "        OPTIONAL { ?product rdfs:label ?label }\n",
        "    }\n",
        "    LIMIT 10 # Limita los resultados para no mostrar demasiados\n",
        "    \"\"\"\n",
        "\n",
        "    # Ejecutar la consulta\n",
        "    results1 = g.query(query1, initNs={\"ex\": ex, \"schema\": schema, \"rdfs\": RDFS}) # initNs vincula los prefijos usados en la consulta\n",
        "\n",
        "    # Procesar y mostrar los resultados\n",
        "    print(\"Usuarios y Productos con Sentimiento Positivo (Primeros 10):\")\n",
        "    for row in results1:\n",
        "        user_uri = row.user # El resultado es un objeto Term (URI, Literal, etc.)\n",
        "        product_uri = row.product\n",
        "        label = row.label # Puede ser None si no hay etiqueta rdfs:label\n",
        "\n",
        "        # Puedes acceder al valor de la URI o Literal\n",
        "        print(f\"  Usuario: {user_uri.split('#')[-1]}, Producto URI: {product_uri.split('#')[-1]}, Etiqueta: {label if label else 'N/A'}\")\n",
        "\n",
        "\n",
        "    # --- Consulta 2: Obtener todos los productos comprados por un usuario específico (ej: 'user456') ---\n",
        "    # Reemplaza 'user456' con un username que esperes encontrar en tus datos\n",
        "    print(\"\\n--- Consulta 2: Productos comprados por un usuario específico ---\")\n",
        "    target_username = \"Orion8881118\" # <--- Reemplaza con un username real de tus datos\n",
        "    target_user_uri = ex[target_username.replace(\" \", \"_\").replace(\".\", \"_\")] # Crear la URI para el usuario objetivo\n",
        "\n",
        "    # Consulta SPARQL\n",
        "    query2 = f\"\"\"\n",
        "    SELECT ?product ?label\n",
        "    WHERE {{\n",
        "        <{target_user_uri}> ex:bought ?product . # Buscar tripletas donde el sujeto es la URI del usuario objetivo y el predicado es ex:bought\n",
        "        OPTIONAL {{ ?product rdfs:label ?label }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    results2 = g.query(query2, initNs={\"ex\": ex, \"rdfs\": RDFS})\n",
        "\n",
        "    print(f\"Productos comprados por '{target_username}':\")\n",
        "    if len(results2) == 0:\n",
        "        print(f\"  No se encontraron productos comprados por '{target_username}'.\")\n",
        "    else:\n",
        "        for row in results2:\n",
        "            product_uri = row.product\n",
        "            label = row.label\n",
        "            print(f\"  Producto URI: {product_uri.split('#')[-1]}, Etiqueta: {label if label else 'N/A'}\")\n",
        "\n",
        "\n",
        "    # --- Consulta 3: Obtener todas las relaciones donde el objeto es un producto específico (ej: 'Kindle Paperwhite') ---\n",
        "    # Reemplaza 'Kindle Paperwhite' con un nombre base de producto que esperes encontrar\n",
        "    print(\"\\n--- Consulta 3: Relaciones donde el objeto es un producto específico ---\")\n",
        "    target_product_name = \"Kindle Paperwhite\" # <--- Reemplaza con un nombre base de producto real\n",
        "    target_product_uri = ex[target_product_name.replace(\" \", \"_\").replace(\".\", \"_\")] # Crear la URI para el producto objetivo\n",
        "\n",
        "    # Consulta SPARQL\n",
        "    query3 = f\"\"\"\n",
        "    SELECT ?subject ?predicate\n",
        "    WHERE {{\n",
        "        ?subject ?predicate <{target_product_uri}> . # Buscar tripletas donde el objeto es la URI del producto objetivo\n",
        "        FILTER (?predicate != ex:bought) # Excluir la relación de compra implícita si no te interesa\n",
        "    }}\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "\n",
        "    results3 = g.query(query3, initNs={\"ex\": ex})\n",
        "\n",
        "    print(f\"Relaciones donde el objeto es '{target_product_name}' (Primeras 10, excluyendo 'bought'):\")\n",
        "    if len(results3) == 0:\n",
        "         print(f\"  No se encontraron relaciones para '{target_product_name}' (excluyendo 'bought').\")\n",
        "    else:\n",
        "        for row in results3:\n",
        "            subject = row.subject\n",
        "            predicate = row.predicate\n",
        "            print(f\"  Sujeto: {subject.split('#')[-1] if isinstance(subject, URIRef) else subject}, Predicado: {predicate.split('#')[-1] if isinstance(predicate, URIRef) else predicate}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj7Uw6vd7cdD",
        "outputId": "627f44f9-317b-4b93-ae21-65e50d20d06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Grafo RDF cargado exitosamente desde 'reviews_knowledge_graph.ttl'.\n",
            "Total de tripletas en el grafo cargado: 131378\n",
            "\n",
            "--- Consulta 1: Productos con Sentimiento Positivo ---\n",
            "Usuarios y Productos con Sentimiento Positivo (Primeros 10):\n",
            "  Usuario: ., Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "  Usuario: A_M, Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "  Usuario: Alan_Kiphut, Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "  Usuario: Amazon_Customer, Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "  Usuario: Amazon_CustomerCharlie, Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "  Usuario: Amber_V., Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "  Usuario: Angel_Gabriel_Pedretti, Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "  Usuario: Angela_N._Payne, Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "  Usuario: ArthurAmazon_Customer, Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "  Usuario: B.B.L, Producto URI: Amazon_5W_Usb_Official_Oem_Charger_And_Power_Adapter_For_Fire_Tablets_And_Kindle_Ereaders, Etiqueta: Amazon 5W Usb Official Oem Charger And Power Adapter For Fire Tablets And Kindle Ereaders\n",
            "\n",
            "--- Consulta 2: Productos comprados por un usuario específico ---\n",
            "Productos comprados por 'Orion8881118':\n",
            "  Producto URI: Fire_Hd_8_Tablet, Etiqueta: Fire Hd 8 Tablet\n",
            "\n",
            "--- Consulta 3: Relaciones donde el objeto es un producto específico ---\n",
            "Relaciones donde el objeto es 'Kindle Paperwhite' (Primeras 10, excluyendo 'bought'):\n",
            "  Sujeto: Aimee, Predicado: has_positive_sentiment_towards\n",
            "  Sujeto: Aimee, Predicado: love\n",
            "  Sujeto: Aimee, Predicado: recommends\n",
            "  Sujeto: Edward, Predicado: has_positive_sentiment_towards\n",
            "  Sujeto: Edward, Predicado: recommends\n",
            "  Sujeto: GBmarbles, Predicado: has_negative_sentiment_towards\n",
            "  Sujeto: GBmarbles, Predicado: recommends\n",
            "  Sujeto: Heed, Predicado: has_positive_sentiment_towards\n",
            "  Sujeto: Heed, Predicado: like\n",
            "  Sujeto: Heed, Predicado: recommends\n"
          ]
        }
      ]
    }
  ]
}